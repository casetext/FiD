{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50bf900d-2f90-4bd3-9002-e5bff7e5c4bc",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051e6be8-705f-4144-8de5-b875a61fcd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "import argparse\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47371a17-4b5d-4d76-b8d9-84be8931b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/divy/FiD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5550822f-9127-4a09-a2b7-459f7a261be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "from src.data import load_data\n",
    "from src.evaluation import ems\n",
    "import src.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0755d-d508-416c-b4aa-8045b6a2eaae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a2dcfff-9a85-42a2-9bc7-630e8380468e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5643fc-1da6-4463-b210-f202c713aa7a",
   "metadata": {},
   "source": [
    "Try out NQ with NQ pretrained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03358bdf-f635-4c32-97af-64598b352a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = load_data(\n",
    "                            \"/mnt/disks/external_mounted_disk/datasets/NQ/NQ/dev.json\",\n",
    "                            global_rank=0,\n",
    "                            world_size=1,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f16c95ae-0fa7-4b31-bf78-6c06357ab8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8757"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_passages = 10\n",
    "eval_dataset = src.data.Dataset(eval_examples, n_passages)\n",
    "\n",
    "sampler = SequentialSampler(eval_dataset)\n",
    "tokenizer = transformers.T5Tokenizer.from_pretrained('t5-base')\n",
    "collator = src.data.Collator(200, tokenizer, answer_maxlength=-1)\n",
    "\n",
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea37eb3-8337-4a5d-9dd0-1de9e8ef5992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(eval_dataset,\n",
    "        sampler=sampler,\n",
    "        batch_size=1,\n",
    "        drop_last=False,\n",
    "        num_workers=10,\n",
    "        collate_fn=collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ce06b4c-6138-427b-a9df-60e7ec89b8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0]), tensor([[16121,  8688,     1]]), tensor([[True, True, True]]), tensor([[[  822,    10,   113, 10159,     7,   405,     3,    88,   333,   140,\n",
      "             28,     3,    60,   115,     9,  2233,    10,  3520,   216,  2129,\n",
      "            148,  2625,    10,  3520,   216,  2129,   148,    96,  4135,    15,\n",
      "              7,   216,  2129,   148,   121,    19,     3,     9,  2324,  1545,\n",
      "             57, 20655, 25649,    11, 17724,  5500,  7059,     6,    11,  4381,\n",
      "             38,     3,     9,   788,    17,    57,   797,   684,   723,  3153,\n",
      "          22789,     9,  3038, 16924,  2060,    11, 16121,  8688,     5,    94,\n",
      "             47,  1883,    16,  1660,  8388,    38,     8,   166,   712,    45,\n",
      "          22789,     9,    31,     7,  2306,    96, 24189,   222, 11436,     7,\n",
      "          14816,  2759,  1280,    94,    19,    80,    13,   684,   723,    31,\n",
      "              7,   633,  3605,    81,     3,     9,   333, 19938,     5,    96,\n",
      "           4135,    15,     7,   216,  2129,   148,   121,    47,  1545,    16,\n",
      "          14505,    57, 17724,  5500,  7059,     5,   216,  4381,    34,    28,\n",
      "              3,     9, 17204,    16,    84,     3,    88,  3032,    44,     8,\n",
      "             97,     6,   250,     3,    88,  1114,     3,     9,  2324,    24,\n",
      "            228,    36,     3,     7,   425,    57,     8,   119,   192,   724,\n",
      "              1]]]), tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True]]]))\n"
     ]
    }
   ],
   "source": [
    "for i, b in enumerate(dataloader):\n",
    "    print(b)\n",
    "    if i >= 0:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac1095f-8d4f-4efd-9724-e1f42a5e2a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "212013b8-a939-48f8-b484-f6c55b8cb7ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96036544-f6de-40a2-b36e-4bdd00951c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 't5-base'\n",
    "model_class = src.model.FiDT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f0b666-335b-4461-bb9c-820e2e0d30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"/home/divy/FiD/model_ckpts/nq_reader_base\"\n",
    "model = model_class.from_pretrained(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65cbcb66-00ea-447d-b253-0405630114fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda692ca-5547-4f5f-aceb-dedc4f8281eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68f6978d-6e27-4c24-a2e3-5e974ead8b29",
   "metadata": {},
   "source": [
    "### load an example input, generated output and ground truth output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a7d8747d-e766-428a-a76f-71058148f089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([52])\n",
      "torch.Size([1, 10, 190])\n",
      "====\n",
      "0\n",
      "question: who does the voice for love island australia title: Love Island Australia context: Love Island Australia Love Island Australia is an Australian dating reality show based on the British series \"Love Island\". The series is presented by Sophie Monk and narrated by Eoghan McDermott. The series began airing on 9Go! and 9Now on 27 May 2018. The final aired on 5 July 2018, with Grant Crapp and Tayla Damir winning and sharing the $50,000 prize money. Eden Dally and Erin Barnett finished as runners up. The series also airs in the United Kingdom on ITVBe and in the Republic of Ireland on 3e, and is available in New Zealand on TVNZ OnDemand. Love</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "====\n",
      "1\n",
      "question: who does the voice for love island australia title: Love Island Australia context: from Nine's streaming service, 9Now. The series performed strongest in the youngest key demographic band (16 to 39 year olds), but ranked low amongst total viewers. The series generated in excess of 150 million views through its official YouTube channel, however 85% were viewers from outside Australia, and YouTube viewers cannot be monetised. Love Island Australia Love Island Australia is an Australian dating reality show based on the British series \"Love Island\". The series is presented by Sophie Monk and narrated by Eoghan McDermott. The series began airing on 9Go! and 9Now on 27 May 2018. The final aired on</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "====\n",
      "2\n",
      "question: who does the voice for love island australia title: Eoghan McDermott context: Eoghan McDermott Eoghan McDermott (also known as Eoghan Mac Diarmada; born 15 April 1983) is an Irish television and radio presenter mostly known for hosting \"The Voice of Ireland\" in Ireland, Xfm Drivetime in the UK and narrating Love Island Australia. In addition to being a radio presenter with RTE 2FM, McDermott is also the current M.C. for the Choice Music Prize, replacing Today FM radio presenter Paul McLoone who served as the previous M.C. for the main Choice Music Prize awards ceremony. McDermott was born in Limerick but grew up in Dublin. His father is Kevin McDermott, an author.</s>\n",
      "====\n",
      "3\n",
      "question: who does the voice for love island australia title: Love Island Australia context: Island Australia will return for a second season in 2019, and will seek contestants with tap dancing skills. The new season will air on Nine's primary channel, with the series expected to move from Spain and to a new location. \"Love Island\" involves a group of contestants, referred to as Islanders, living in isolation from the outside world in a villa in Mallorca, Spain, constantly under video surveillance. To survive in the villa the Islanders must be coupled up with another Islander, whether it be for love, friendship or money, as the overall winning couple receives $50,000. On the first</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "====\n",
      "4\n",
      "question: who does the voice for love island australia title: Love Island (2015 TV series) context: a German version premiered in RTL II. An Australian version was commissioned by the Nine Network for its secondary channel 9Go!. On 8 August 2018 CBS ordered an American version of Love Island. Legend: In 2018, \"Love Island\" received the BAFTA TV Award for the 'Best Reality and Constructed Factual' category. Love Island (2015 TV series) Love Island is a British dating reality show. It is a revival of the earlier series of the same name, which aired for two series in 2005 and 2006 on ITV. The series is presented by Caroline Flack, and is narrated by Iain Stirling.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "====\n",
      "5\n",
      "question: who does the voice for love island australia title: Eoghan McDermott context: it was announced that he would stand in for Louise McSharry on the station's weekday evening slot for the next few months while she receives treatment for cancer. In September 2015, McDermott took over from the Driveby show from Colm Hayes under a two-year contract. He costarred on an episode of West Cork FM. Eoghan McDermott Eoghan McDermott (also known as Eoghan Mac Diarmada; born 15 April 1983) is an Irish television and radio presenter mostly known for hosting \"The Voice of Ireland\" in Ireland, Xfm Drivetime in the UK and narrating Love Island Australia. In addition to being a</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "====\n",
      "6\n",
      "question: who does the voice for love island australia title: Love Island Australia context: twist has occurred where it has been up to the Islanders to vote one of their own off the island. During the final week, the public vote for which couple they want to win the series and therefore take home $50,000. The Islanders for the first series were released by Nine on 21 May 2018, just one week before the premiere episode. The main events in the \"Love Island\" villa are summarised in the table below. \"Love Island Australia\" averaged around 200,000 viewers for its linear broadcasts in overnight OzTAM ratings, increasing to an average of 511,000 when adding viewers</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "====\n",
      "7\n",
      "question: who does the voice for love island australia title: Eoghan McDermott context: and mind. In 2015, he hosted the Irish selection shows for the Junior Eurovision Song Contest 2015 (a role he continued in 2016), and filmed a pilot episode for a UK game show in Paris, which was made by the team behind Gogglebox. McDermott is the narrator for the television series Love Island Australia. McDermott has played the lead role of \"DJ Pete\" in all four series of the joint TG4/BBC Northern Ireland production, \"Seacht\", a teen drama. McDermott completed his dance training at the Broadway Dance Center and has worked as a dancer and choreographer, supporting acts such as</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "====\n",
      "8\n",
      "question: who does the voice for love island australia title: Love Island (2015 TV series) context: villa after being voted back in by the public; a first for the show. The series ended on 24 July 2017 and was won by Amber Davies and Kem Cetinay. During the final \"Aftersun\" episode of the series, which was presented live from outside the villa, it was announced that there would be a one-off reunion special airing on 30 July 2017 which will include all of the Islanders from the series. The fourth series began on 4 June 2018, and launched with a record 4,050,000 viewers making it the most watched multichannel TV programme since the 2012 Summer Olympics</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "====\n",
      "9\n",
      "question: who does the voice for love island australia title: Love It or List It Australia context: Editor-in-chief of Belle magazine. Currently Editor-at-large at Vogue Living, Neale's frank honesty, warm humour and British eccentricities have charmed audiences in his role of expert judge on The Block. Love It or List It Australia Love It or List It Australia is an Australian TV series, based on the Canadian program of the same name, which began airing on LifeStyle on 27 September 2017. The series is hosted by Andrew Winter (Selling Houses Australia) and Neale Whitaker (The Block; Editor of Vogue Living). It is produced exclusively for Foxtel by Beyond Productions. In October 2017, the series was renewed for</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "\n",
      "generated answer:\n",
      "<pad> Eoghan McDermott</s>\n",
      "\n",
      "\n",
      "ground truth answer:\n",
      "Eoghan McDermott</s>\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    (idx, labels, _, context_ids, context_mask) = batch\n",
    "    \n",
    "    if i == 52:\n",
    "        print(idx)\n",
    "        print(context_ids.shape)\n",
    "        #print(context_ids[0][0])\n",
    "        for j in range(context_ids.shape[1]):\n",
    "\n",
    "            print(\"====\")\n",
    "            print(j)\n",
    "            context = tokenizer.decode(context_ids[0][j])\n",
    "            print(context)\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"generated answer:\")\n",
    "        generated_output = model.generate(\n",
    "                input_ids=context_ids.cuda(),\n",
    "                attention_mask=context_mask.cuda(),\n",
    "                max_length=50\n",
    "            ).cpu()\n",
    "\n",
    "        human_readable_generated_output = tokenizer.decode(generated_output[0])\n",
    "        print(human_readable_generated_output)\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"ground truth answer:\")\n",
    "\n",
    "        print(tokenizer.decode(labels[0]))\n",
    "\n",
    "        forward = model(\n",
    "                input_ids=context_ids.cuda(),\n",
    "                attention_mask=context_mask.cuda(),\n",
    "                decoder_input_ids=labels.cuda(),\n",
    "                output_attentions=True,\n",
    "                output_unnormalized_attentions=True\n",
    "            )           \n",
    "    \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d4b93-c57e-4ed8-8d75-1296995228a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19a0192f-2449-42ad-916c-ea61699edaf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extract cross attention matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "105c5b39-17a1-4a8a-bdd4-1d02461e26f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(forward.cross_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a45db957-6a87-40b0-b560-efeef5c760dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12, 3, 880])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_forward_attentions = torch.cat(forward.cross_attentions, dim=0)\n",
    "stacked_forward_attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c4e273d8-276c-45a8-9868-41d1c40a30ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-80681824., device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(stacked_forward_attentions[:, :, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e089fd6-621e-44d8-bb16-a2a9057dbc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 3, 322])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward.cross_attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26e96656-2048-41eb-8dab-7b4a9dd7ca17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-27950314., device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(forward.cross_attentions[0][:, :, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "074aca81-f1bd-49be-acdd-c24472bca22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 3, 322])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward.cross_attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcbf0b37-bcc0-45f6-a6cc-c4df37e5b143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(forward.cross_attentions[3], dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce7b6408-4303-4525-948a-d0c67d47b002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.0000e+09, -9.0000e+09, -9.0000e+09],\n",
       "         [-9.0000e+09, -9.0000e+09, -9.0000e+09],\n",
       "         [-9.0000e+09, -9.0000e+09, -9.0000e+09],\n",
       "         [-9.0000e+09, -9.0000e+09, -9.0000e+09],\n",
       "         [-9.0000e+09, -9.0000e+09, -9.0000e+09],\n",
       "         [-9.0000e+09, -9.0000e+09, -9.0000e+09],\n",
       "         [-9.0000e+09, -9.0000e+09, -9.0000e+09],\n",
       "         [-9.0000e+09, -9.0000e+09, -9.0000e+09],\n",
       "         [-9.0000e+09, -9.0000e+09, -9.0000e+09],\n",
       "         [-9.0000e+09, -9.0000e+09, -9.0000e+09],\n",
       "         [-9.0000e+09, -9.0000e+09, -9.0000e+09],\n",
       "         [-9.0000e+09, -9.0000e+09, -9.0000e+09]]], device='cuda:0',\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(forward.cross_attentions[0], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ae0b9ab-e880-4d51-86fc-f25981a4c944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9729e+00,  1.3851e+00, -2.1832e+00, -1.1376e+00, -1.2776e+00,\n",
       "        -3.4125e+00, -5.3054e+00, -4.8791e+00, -3.2870e+00, -4.4425e+00,\n",
       "        -1.8687e+00, -2.5599e+00, -2.6223e+00, -4.1923e+00, -1.3342e+00,\n",
       "        -5.2764e-01,  5.1298e-01, -4.1823e+00, -4.5534e+00, -3.5556e+00,\n",
       "        -4.6796e+00, -8.2563e-01,  9.5785e-01, -3.3178e+00, -3.6698e+00,\n",
       "        -3.0187e+00, -4.7601e+00, -4.2497e+00, -2.8946e+00, -5.3650e+00,\n",
       "        -3.4801e+00, -4.4688e+00, -3.2278e+00, -4.8753e+00, -3.7288e+00,\n",
       "        -1.9606e+00,  1.3321e+00, -3.6053e+00, -1.8468e+00, -4.1075e+00,\n",
       "        -2.9598e+00, -2.5535e+00, -2.2707e+00, -3.5604e+00, -3.8036e+00,\n",
       "        -2.4402e+00, -4.0344e+00, -2.8101e+00, -5.1451e+00, -1.9998e+00,\n",
       "        -1.9949e+00,  1.3623e+00, -9.9303e-01,  8.5180e-01, -2.4537e-01,\n",
       "        -1.4495e+00, -1.2180e+00,  5.8565e-01,  6.4136e-01,  1.1378e+00,\n",
       "        -8.3264e-01, -1.4387e+00, -1.5302e+00, -1.3770e+00, -8.3234e-01,\n",
       "        -5.2860e-01,  2.6433e+00,  3.8919e-01, -9.3304e-01, -2.2436e+00,\n",
       "        -4.4312e+00, -3.5490e+00, -4.6887e+00, -2.9122e+00, -4.8157e+00,\n",
       "        -4.0654e+00, -4.5377e+00, -4.3426e+00, -1.8697e+00, -4.4618e+00,\n",
       "        -2.1643e+00, -2.6291e+00, -5.6696e+00, -5.3164e+00, -4.3781e+00,\n",
       "        -5.9670e+00, -5.2321e+00, -4.2080e+00, -3.1265e+00, -4.2005e+00,\n",
       "        -2.8771e+00, -3.2956e+00, -1.1805e+00, -2.8303e+00, -4.4768e+00,\n",
       "        -5.4440e+00, -6.1873e+00, -1.0963e+00, -1.8426e+00, -5.2303e+00,\n",
       "        -6.4775e+00, -5.9651e+00, -3.0967e+00, -4.5600e+00,  1.3282e+00,\n",
       "        -5.9412e+00, -2.8950e+00, -6.2434e+00, -1.8653e-01, -5.2957e+00,\n",
       "        -4.1540e+00, -5.3873e+00, -4.4140e+00, -5.1191e+00, -3.9336e+00,\n",
       "        -4.8321e+00, -3.9711e+00, -4.4869e+00, -2.6614e+00, -4.1897e+00,\n",
       "        -4.4527e+00, -1.5985e+00, -3.6627e+00, -3.5549e+00, -4.6487e+00,\n",
       "        -3.2182e+00, -3.3348e+00, -4.3084e+00, -5.2264e+00, -6.0960e+00,\n",
       "         1.3176e+00, -5.0884e+00, -4.8702e+00, -6.5156e+00, -4.8602e+00,\n",
       "         1.2623e+00, -2.1398e+00, -4.7851e+00, -5.9398e+00, -4.7313e+00,\n",
       "        -4.0941e+00, -3.6307e+00, -5.4165e+00,  1.3143e+00, -5.5277e+00,\n",
       "        -5.8773e+00,  1.3321e+00, -7.3642e+00, -3.1572e+00, -7.0340e+00,\n",
       "        -5.5194e+00, -6.1182e+00,  1.2496e+00, -4.2643e+00, -2.5299e+00,\n",
       "        -2.6888e+00, -3.7361e+00, -2.4774e+00, -3.2266e+00, -2.0169e+00,\n",
       "         1.2179e+00,  1.9280e+00,  1.3883e+00, -2.2823e+00, -8.9481e-01,\n",
       "        -1.3417e+00, -2.5561e+00, -4.1483e+00, -3.2045e+00, -2.3281e+00,\n",
       "        -3.7932e+00, -2.6662e+00, -3.6371e+00, -4.3229e+00, -4.6901e+00,\n",
       "        -2.7355e+00,  3.5384e-01,  1.5543e+00,  2.6047e+00, -2.9533e-01,\n",
       "         1.3160e+00,  1.8552e+00,  2.4846e+00, -1.4129e+00,  2.7614e+00,\n",
       "        -1.2821e+00, -7.3053e-01, -5.7694e-01, -4.0411e+00, -4.8368e+00,\n",
       "        -3.8859e+00, -4.3150e+00, -4.2504e+00, -3.5745e-01, -3.5462e-01,\n",
       "        -7.3754e-01, -1.9055e+00, -3.0862e-01, -2.6462e-01, -8.4304e-01,\n",
       "        -3.4324e-01, -5.9995e+00, -6.0510e+00,  1.4091e+00, -5.7798e+00,\n",
       "        -4.3337e+00, -6.5568e+00,  1.4036e+00, -5.8580e+00, -3.3419e+00,\n",
       "        -2.4596e+00, -4.8135e+00,  8.3057e-01, -6.2245e+00, -5.9418e+00,\n",
       "        -4.8557e+00, -2.4293e+00, -2.9622e+00, -4.5057e+00, -6.1893e+00,\n",
       "        -5.0973e+00, -3.5218e+00, -5.3923e+00, -4.5799e+00, -3.9574e+00,\n",
       "        -4.4635e+00, -4.5057e+00, -1.5150e+00, -2.8114e+00, -4.3329e+00,\n",
       "        -3.5980e+00, -3.8420e+00, -1.8371e+00, -1.9484e-01, -5.4847e+00,\n",
       "        -1.4760e+00, -3.6981e+00, -4.9489e+00, -4.2241e+00, -2.5071e-01,\n",
       "        -4.3569e+00, -4.0960e+00, -6.1977e+00, -5.1910e+00, -3.2393e+00,\n",
       "        -4.4211e+00, -5.0790e+00, -3.4128e+00, -5.0038e+00, -4.9638e+00,\n",
       "        -5.1276e+00, -4.3316e+00, -4.1899e+00, -3.3087e+00, -3.0998e+00,\n",
       "        -1.7585e+00, -8.6455e-01, -4.9921e+00, -2.4460e+00, -4.3285e+00,\n",
       "        -5.2264e+00, -4.2715e+00, -2.9195e+00, -3.7304e+00, -2.3466e+00,\n",
       "        -3.2093e+00, -1.2309e+00, -2.9737e+00, -2.6926e+00,  3.8574e-01,\n",
       "        -1.8895e+00,  1.1804e+00, -1.4196e+00, -3.3326e+00, -1.6068e+00,\n",
       "        -4.0972e-01, -5.0661e-01, -1.4656e+00, -7.8192e-01, -3.3248e+00,\n",
       "        -4.1226e+00, -1.3924e+00, -1.6840e+00, -2.9675e+00, -4.3076e+00,\n",
       "        -4.3947e+00, -4.6235e+00, -4.1214e+00, -4.3019e+00, -3.6414e+00,\n",
       "        -2.2205e+00, -8.0839e-01, -3.1549e+00, -4.5909e+00,  3.8164e-01,\n",
       "         1.3151e-01,  4.4845e-01,  1.4407e+00, -3.2950e+00, -9.5082e-02,\n",
       "        -1.9994e+00, -2.3387e+00, -3.9092e-01, -5.1621e-01, -1.2550e+00,\n",
       "        -8.8810e-01,  1.1211e-01, -2.4601e+00, -5.3425e+00, -2.0550e+00,\n",
       "        -3.2892e+00, -4.2305e+00,  1.2666e+00, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward.cross_attentions[0][0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4dd3613a-0810-473a-921e-6d0b19e79c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10,\n",
       "          -2.4000e+10, -2.4000e+10],\n",
       "         [-2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10,\n",
       "          -2.4000e+10, -2.4000e+10],\n",
       "         [-2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10,\n",
       "          -2.4000e+10, -2.4000e+10],\n",
       "         [-2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10,\n",
       "          -2.4000e+10, -2.4000e+10],\n",
       "         [-2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10,\n",
       "          -2.4000e+10, -2.4000e+10],\n",
       "         [-2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10,\n",
       "          -2.4000e+10, -2.4000e+10],\n",
       "         [-2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10,\n",
       "          -2.4000e+10, -2.4000e+10],\n",
       "         [-2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10,\n",
       "          -2.4000e+10, -2.4000e+10],\n",
       "         [-2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10,\n",
       "          -2.4000e+10, -2.4000e+10],\n",
       "         [-2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10,\n",
       "          -2.4000e+10, -2.4000e+10],\n",
       "         [-2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10,\n",
       "          -2.4000e+10, -2.4000e+10],\n",
       "         [-2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10, -2.4000e+10,\n",
       "          -2.4000e+10, -2.4000e+10]]], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(forward.cross_attentions[0], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05c654-5b06-48bf-a17e-d8958efcf1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "664f7ee5-85d2-4281-9061-349867efbb45",
   "metadata": {},
   "source": [
    "## compute g_{q,p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "acfa0045-24b1-47d8-af71-fbd007047713",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_forward_attentions = torch.cat(forward.cross_attentions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "30359bc1-96d6-42da-9ffc-c29d8e5a36c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12, 3, 2000])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_forward_attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "03b01390-ef52-4d16-a86f-c102473d23c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 200])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ca18e367-24b1-4afe-966d-0afe39ef5a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2000])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk = torch.reshape(context_mask, (1, context_mask.shape[1]*context_mask.shape[2])).cuda()\n",
    "msk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b795e842-0b16-4fe6-a39e-335b179dbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_stacked_forward_attentions = stacked_forward_attentions.masked_fill(msk == False, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6454fc41-d968-4de2-91c5-b6b65fd24879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12, 3, 2000])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_stacked_forward_attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "e7f83c82-3b7e-4772-bc49-0ad4ade21800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([12, 12, 3, 200])\n",
      "tensor(-2.9641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "============\n",
      "1\n",
      "torch.Size([12, 12, 3, 200])\n",
      "tensor(-2.4551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "============\n",
      "2\n",
      "torch.Size([12, 12, 3, 200])\n",
      "tensor(-2.2811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "============\n",
      "3\n",
      "torch.Size([12, 12, 3, 200])\n",
      "tensor(-2.5580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "============\n",
      "4\n",
      "torch.Size([12, 12, 3, 200])\n",
      "tensor(-3.2703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "============\n",
      "5\n",
      "torch.Size([12, 12, 3, 200])\n",
      "tensor(-3.2976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "============\n",
      "6\n",
      "torch.Size([12, 12, 3, 200])\n",
      "tensor(-2.7875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "============\n",
      "7\n",
      "torch.Size([12, 12, 3, 200])\n",
      "tensor(-3.5087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "============\n",
      "8\n",
      "torch.Size([12, 12, 3, 200])\n",
      "tensor(-2.2828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "============\n",
      "9\n",
      "torch.Size([12, 12, 3, 200])\n",
      "tensor(-2.5195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "============\n"
     ]
    }
   ],
   "source": [
    "for i in range(context_mask.shape[1]):\n",
    "    print(i)\n",
    "    sliced_masked_stacked_forward_attentions = masked_stacked_forward_attentions[:, :, :, (i*context_mask.shape[2]):((i + 1)*context_mask.shape[2])]\n",
    "    print(sliced_masked_stacked_forward_attentions.shape)\n",
    "    g_qp = torch.mean(sliced_masked_stacked_forward_attentions[:, :, 0, :])\n",
    "    print(g_qp)\n",
    "    print(\"============\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d1145e82-be68-4168-a3ee-a8723c5e23c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-2.4053),\n",
       " tensor(-2.3345),\n",
       " tensor(-2.7575),\n",
       " tensor(-2.5900),\n",
       " tensor(-2.4345),\n",
       " tensor(-2.6038),\n",
       " tensor(-2.0521),\n",
       " tensor(-2.8895),\n",
       " tensor(-2.0617),\n",
       " tensor(-2.1865)]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_qp = model.obtain_gqp(\n",
    "    input_ids=context_ids.cuda(),\n",
    "    attention_mask=context_mask.cuda(),\n",
    "    decoder_input_ids=labels.cuda(),\n",
    "    output_attentions=True,\n",
    "    output_unnormalized_attentions=True\n",
    ")\n",
    "\n",
    "g_qp = [el.detach() for el in g_qp]\n",
    "\n",
    "g_qp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e147e00b-f1d7-480b-939d-114cc03ea773",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_attention_matrix = torch.mean(masked_stacked_forward_attentions, dim = (0, 1)).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6670f54-d07d-4aea-96d6-9fc5acc5d351",
   "metadata": {},
   "source": [
    "### Visualise layer-head averaged cross-attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "7c133057-ae3b-4b9c-9af8-7421d0c827ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7e240e1700>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20, 10), facecolor=\"white\")\n",
    "ax = fig.add_subplot()\n",
    "#ax.imshow(averaged_attention_matrix[:, :], cmap='hot', interpolation='nearest')\n",
    "ax.matshow(averaged_attention_matrix[:2, :], aspect='auto', vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "34e1fb19-e021-402a-a208-209deeae29e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 166])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averaged_attention_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee404de-5893-41a3-a113-75927b1cafb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe4549cd-3213-4a2c-931b-5f986b1b5ad9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Obtaining the generative output score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "764c5d4b-2536-4871-9f90-baeaf5668f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training example: 78\n",
      "tensor([[    0, 16655,  2570,   115,     7,     1]], device='cuda:0')\n",
      "<pad> Sean Combs</s>\n",
      "['Suzanne Siegel']\n"
     ]
    }
   ],
   "source": [
    "# only one context vector\n",
    "for i, batch in enumerate(dataloader):\n",
    "    \n",
    "    if i == 78:\n",
    "        print(f\"training example: {i}\")\n",
    "        (idx, _, _, context_ids, context_mask) = batch\n",
    "        sequences = model.generate(\n",
    "                    input_ids=context_ids.cuda(),\n",
    "                    attention_mask=context_mask.cuda(),\n",
    "                    max_length=10,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True\n",
    "                ).sequences\n",
    "\n",
    "        print(sequences)\n",
    "        for k, o in enumerate(sequences):\n",
    "            ans = tokenizer.decode(o, skip_special_tokens=False)\n",
    "            gold = eval_dataset.get_example(idx[k])['answers']\n",
    "            score = ems(ans, gold)\n",
    "            # total += 1\n",
    "            # exactmatch.append(score)\n",
    "            print(ans)\n",
    "            print(gold) \n",
    "            \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d4fc799d-b658-43a9-97de-e529d4250ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.9990, device='cuda:0')\n",
      "tensor(-0.0031, device='cuda:0')\n",
      "tensor(-2.1935e-05, device='cuda:0')\n",
      "tensor(-7.8800e-05, device='cuda:0')\n",
      "tensor(-0.0035, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0057, device='cuda:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "total_log_prob = 0\n",
    "for i in range(len(outputs.scores)):\n",
    "    print(torch.log(torch.max(softmax(outputs.scores[i]))))\n",
    "    total_log_prob -= torch.log(torch.max(softmax(outputs.scores[i])))\n",
    "\n",
    "total_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a379b-7e46-4e00-9cf7-48f81571934c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f52b13-94a2-4043-b07d-1195e05255dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f66611c-d8a9-42d5-83e6-298ec5f025bf",
   "metadata": {},
   "source": [
    "## Comparing tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f3bea34-717f-4adc-8713-5fc46c51c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "t5_tokenizer = transformers.T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86de111e-f083-4537-bba7-cefb87414e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = \"Welcome to the HuggingFace Library\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f25ba5fc-1856-4ee2-a02c-1b4cb2a32e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  6160,  2000,  1996, 17662, 12172,  3075,   102]])\n",
      "===\n",
      "tensor([[ 5242,    12,     8, 11560,  3896,   371,  3302,  5355,     1]])\n"
     ]
    }
   ],
   "source": [
    "bert_tokenized = bert_tokenizer(input_sequence, return_tensors='pt')\n",
    "t5_tokenized = t5_tokenizer(input_sequence, return_tensors='pt')\n",
    "\n",
    "print(bert_tokenized.input_ids)\n",
    "print(\"===\")\n",
    "print(t5_tokenized.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ada9e88-a19c-46e9-a479-950b78a2bfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] welcome to the huggingface library [SEP]\n",
      "===\n",
      "Welcome to the HuggingFace Library</s>\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.decode(bert_tokenized.input_ids[0]))\n",
    "print(\"===\")\n",
    "print(t5_tokenizer.decode(t5_tokenized.input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b59e9e3-c830-431d-a41b-bb4dc8568b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'welcome', 'to', 'the', 'hugging', '##face', 'library', '[SEP]']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.convert_ids_to_tokens(bert_tokenized.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e35906e4-9b0f-453e-be19-04572a2196a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Welcome', '▁to', '▁the', '▁Hug', 'ging', 'F', 'ace', '▁Library', '</s>']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.convert_ids_to_tokens(t5_tokenized.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d841dcf5-03f4-45b3-833c-f08068432109",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn((2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75b9d800-96a1-4530-a0aa-a5fafa60720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = torch.tensor([[True, False, True], [False, True, False]])\n",
    "msk2 = torch.tensor([[True, False, True, True]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbfd9dc2-991a-434f-a0bf-cfa7c5e3513d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1771,  0.8998, -0.6725],\n",
       "        [-1.0138,  1.7796, -0.3884]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94242788-d831-4b1a-9af2-2765cddcfc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3247,  0.0000, -0.0618, -0.7110],\n",
       "        [-0.9271,  0.0000,  0.5299,  0.6540]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.masked_fill(msk2 == 0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66176161-54e4-4b71-a848-01d6ce8e0486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6f450d1b-f2b9-49e1-b9c3-f14d8afe1216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "061ace48-4a47-4b81-a1e5-d981cea097c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "04227314-f4aa-42ad-b782-1ee2659743c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"summarize: Hello, my dog is cute\", return_tensors=\"pt\")  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "bd8f6f04-729c-49b7-b10f-89bd1c1d9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = model.generate(input_ids, return_dict_in_generate=True, output_scores=True).sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "5f823567-73c8-4a55-abc3-adc6487965b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   82, 1782,   19, 5295,   11, 5295,    5,    1]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b3cef76a-d40a-4ea9-b6cc-b8035bcabd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.generate(input_ids, return_dict_in_generate=True, output_scores=True).scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c3e3e2b4-e733-47d3-af40-11982e6f4051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1782)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "fae743d0-70bf-4333-a75d-42cb2bb20ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.6708)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9316a-c101-408f-9be3-2449dc815016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
